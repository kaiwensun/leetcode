# Is gradient descent really correct?

> Source: [731883](https://leetcode.com/problems/best-position-for-a-service-centre/discuss/731883/is-gradient-descent-really-correct)
>
> Created at: 2020-07-12 05:43:21
>
> Updated at: 2020-07-12 06:49:30

----

I have two questions about people\'s gradient descent solutions:

## **1. How to guarantee the optimal point we found satisfies the accuracy requirement?**

> "Answers within `10^-5` of the actual value will be accepted."

I know people using grardient descent can try to set a stop condition by setting a minimal allowed search step on delta x and delta y. However, does it mathematically give us a guarantee?

For example, when doing gradient descent on the one-variable function `f(x) = (10^12) * (x^2)`, if you set the step accuracy `epsilon = 10 ^-6`, then when you are searching around the point `x = 10 ^ -7`, you will never find any points that is better than the current `x`, because
* `f(x) = 0.01`
* `f(x - epsilon) = 0.81`
* `f(x + epsilon) = 1.21`
* `f(x) < f(x - epsilon)` and `f(x) < f(x + epsilon)`. ==> This makes us incorrectly believe we\'ve found the optimal point x that is sufficiently accurate.

However, the optimal point is opviously `x_optimal = 0`. The wrong optimal we found `x = 10^-7` staisfies the `epsilon` condition, but  `f(x) - f(0) = 0.01 > 10 ^ -5`. We fail to satisfy the question\'s accuracy requirement.

The question does **not** even give us a constraints like "the positions of all the customers are always at integer points". This makes it harder to find a qualified `epsilon`.

## **2. Why is there only one local minimum?**
For a given center `(xc, yc)`, If the sum of distances can be re-written in a standard binary quadratic form, such as `f(xc, yc) = a xc^2 + b xc + c yc^2 + d yc + e`, then I can understand why there is only one local minum. But the sum of distances can\'t be re-written into that form. It contains square roots. How to mathematically prove it has a single local minimum?

## **3. What\'s the time complexity of the algorithm?**
Is there an upper bound for the iteration? Can anyone theoretically prove that the gradient descent algorithm won\'t TLE on any valid test case?